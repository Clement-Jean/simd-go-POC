diff --git a/src/cmd/compile/internal/arm64/ssa.go b/src/cmd/compile/internal/arm64/ssa.go
index 27b4e881c0..a13a070ed5 100644
--- a/src/cmd/compile/internal/arm64/ssa.go
+++ b/src/cmd/compile/internal/arm64/ssa.go
@@ -577,6 +577,408 @@ func ssaGenValue(s *ssagen.State, v *ssa.Value) {
 		p.Reg = v.Args[0].Reg()
 		p.To.Type = obj.TYPE_REG
 		p.To.Reg = v.Reg()
+	case ssa.OpARM64LoweredSimdAdd8x16:
+		// VLD1 (R1), [V0.D2]
+		// VLD1 (R2), [V1.D2]
+		// VADD V1.B16, V0.B16, V0.B16
+		// VST1 [V0.D2], (R0)
+
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+
+		vld1 := arm64.AVLD1
+		p := s.Prog(vld1)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r1
+		p.To.Type = obj.TYPE_REGLIST
+		p.To.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		p1 := s.Prog(vld1)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = r2
+		p1.To.Type = obj.TYPE_REGLIST
+		p1.To.Offset = 1&31 | arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		vadd := arm64.AVADD
+		p2 := s.Prog(vadd)
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = arm64.REG_ARNG + (arm64.REG_V1 & 31) + ((arm64.ARNG_16B & 15) << 5)
+		p2.Reg = arm64.REG_ARNG + (arm64.REG_V0 & 31) + ((arm64.ARNG_16B & 15) << 5)
+		p2.To.Type = obj.TYPE_REG
+		p2.To.Reg = arm64.REG_ARNG + (arm64.REG_V0 & 31) + ((arm64.ARNG_16B & 15) << 5)
+
+		vst1 := arm64.AVST1
+		p3 := s.Prog(vst1)
+		p3.From.Type = obj.TYPE_REGLIST
+		p3.From.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+		p3.To.Type = obj.TYPE_MEM
+		p3.To.Reg = r0
+
+	case ssa.OpARM64LoweredSimdSub8x16:
+		// VLD1 (R1), [V0.D2]
+		// VLD1 (R2), [V1.D2]
+		// VSUB V1.B16, V0.B16, V0.B16
+		// VST1 [V0.D2], (R0)
+
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+
+		vld1 := arm64.AVLD1
+		p := s.Prog(vld1)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r1
+		p.To.Type = obj.TYPE_REGLIST
+		p.To.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		p1 := s.Prog(vld1)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = r2
+		p1.To.Type = obj.TYPE_REGLIST
+		p1.To.Offset = 1&31 | arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		vsub := arm64.AVSUB
+		p2 := s.Prog(vsub)
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = arm64.REG_ARNG + (arm64.REG_V1 & 31) + ((arm64.ARNG_16B & 15) << 5)
+		p2.Reg = arm64.REG_ARNG + (arm64.REG_V0 & 31) + ((arm64.ARNG_16B & 15) << 5)
+		p2.To.Type = obj.TYPE_REG
+		p2.To.Reg = arm64.REG_ARNG + (arm64.REG_V0 & 31) + ((arm64.ARNG_16B & 15) << 5)
+
+		vst1 := arm64.AVST1
+		p3 := s.Prog(vst1)
+		p3.From.Type = obj.TYPE_REGLIST
+		p3.From.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+		p3.To.Type = obj.TYPE_MEM
+		p3.To.Reg = r0
+
+	case ssa.OpARM64LoweredSimdSaturatingAddU8x16:
+		// VLD1 (R1), [V0.D2]
+		// VLD1 (R2), [V1.D2]
+		// WORD $0x6e210c00
+		// VST1 [V0.D2], (R0)
+
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+
+		vld1 := arm64.AVLD1
+		p := s.Prog(vld1)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r1
+		p.To.Type = obj.TYPE_REGLIST
+		p.To.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		p1 := s.Prog(vld1)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = r2
+		p1.To.Type = obj.TYPE_REGLIST
+		p1.To.Offset = 1&31 | arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		word := arm64.AWORD
+		p2 := s.Prog(word)
+		p2.To.Type = obj.TYPE_CONST
+		p2.To.Offset = 0x6e210c00 // VQADD V1.B16, V0.B16, V0.B16
+
+		vst1 := arm64.AVST1
+		p3 := s.Prog(vst1)
+		p3.From.Type = obj.TYPE_REGLIST
+		p3.From.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+		p3.To.Type = obj.TYPE_MEM
+		p3.To.Reg = r0
+
+	case ssa.OpARM64LoweredSimdSaturatingAdd8x16:
+		// VLD1 (R1), [V0.D2]
+		// VLD1 (R2), [V1.D2]
+		// WORD $0x4e210c00
+		// VST1 [V0.D2], (R0)
+
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+
+		vld1 := arm64.AVLD1
+		p := s.Prog(vld1)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r1
+		p.To.Type = obj.TYPE_REGLIST
+		p.To.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		p1 := s.Prog(vld1)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = r2
+		p1.To.Type = obj.TYPE_REGLIST
+		p1.To.Offset = 1&31 | arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		word := arm64.AWORD
+		p2 := s.Prog(word)
+		p2.To.Type = obj.TYPE_CONST
+		p2.To.Offset = 0x4e210c00 // VQADD V1.B16, V0.B16, V0.B16
+
+		vst1 := arm64.AVST1
+		p3 := s.Prog(vst1)
+		p3.From.Type = obj.TYPE_REGLIST
+		p3.From.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+		p3.To.Type = obj.TYPE_MEM
+		p3.To.Reg = r0
+
+	case ssa.OpARM64LoweredSimdSaturatingSubU8x16:
+		// VLD1 (R1), [V0.D2]
+		// VLD1 (R2), [V1.D2]
+		// WORD $0x6e212c00
+		// VST1 [V0.D2], (R0)
+
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+
+		vld1 := arm64.AVLD1
+		p := s.Prog(vld1)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r1
+		p.To.Type = obj.TYPE_REGLIST
+		p.To.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		p1 := s.Prog(vld1)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = r2
+		p1.To.Type = obj.TYPE_REGLIST
+		p1.To.Offset = 1&31 | arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		word := arm64.AWORD
+		p2 := s.Prog(word)
+		p2.To.Type = obj.TYPE_CONST
+		p2.To.Offset = 0x6e212c00 // VQSUB V1.B16, V0.B16, V0.B16
+
+		vst1 := arm64.AVST1
+		p3 := s.Prog(vst1)
+		p3.From.Type = obj.TYPE_REGLIST
+		p3.From.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+		p3.To.Type = obj.TYPE_MEM
+		p3.To.Reg = r0
+
+	case ssa.OpARM64LoweredSimdSaturatingSub8x16:
+		// VLD1 (R1), [V0.D2]
+		// VLD1 (R2), [V1.D2]
+		// WORD $0x4e212c00
+		// VST1 [V0.D2], (R0)
+
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+
+		vld1 := arm64.AVLD1
+		p := s.Prog(vld1)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r1
+		p.To.Type = obj.TYPE_REGLIST
+		p.To.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		p1 := s.Prog(vld1)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = r2
+		p1.To.Type = obj.TYPE_REGLIST
+		p1.To.Offset = 1&31 | arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		word := arm64.AWORD
+		p2 := s.Prog(word)
+		p2.To.Type = obj.TYPE_CONST
+		p2.To.Offset = 0x4e212c00 // VQSUB V1.B16, V0.B16, V0.B16
+
+		vst1 := arm64.AVST1
+		p3 := s.Prog(vst1)
+		p3.From.Type = obj.TYPE_REGLIST
+		p3.From.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+		p3.To.Type = obj.TYPE_MEM
+		p3.To.Reg = r0
+
+	case ssa.OpARM64LoweredSimdAnd8x16:
+		// VLD1 (R1), [V0.D2]
+		// VLD1 (R2), [V1.D2]
+		// VAND V1.B16 V0.B16 V0.B16
+		// VST1 [V0.D2], (R0)
+
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+
+		vld1 := arm64.AVLD1
+		p := s.Prog(vld1)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r1
+		p.To.Type = obj.TYPE_REGLIST
+		p.To.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		p1 := s.Prog(vld1)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = r2
+		p1.To.Type = obj.TYPE_REGLIST
+		p1.To.Offset = 1&31 | arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		vand := arm64.AVAND
+		p2 := s.Prog(vand)
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = arm64.REG_ARNG + (arm64.REG_V1 & 31) + ((arm64.ARNG_16B & 15) << 5)
+		p2.Reg = arm64.REG_ARNG + (arm64.REG_V0 & 31) + ((arm64.ARNG_16B & 15) << 5)
+		p2.To.Type = obj.TYPE_REG
+		p2.To.Reg = arm64.REG_ARNG + (arm64.REG_V0 & 31) + ((arm64.ARNG_16B & 15) << 5)
+
+		vst1 := arm64.AVST1
+		p3 := s.Prog(vst1)
+		p3.From.Type = obj.TYPE_REGLIST
+		p3.From.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+		p3.To.Type = obj.TYPE_MEM
+		p3.To.Reg = r0
+
+	case ssa.OpARM64LoweredSimdOr8x16:
+		// VLD1 (R1), [V0.D2]
+		// VLD1 (R2), [V1.D2]
+		// VORR V1.B16 V0.B16 V0.B16
+		// VST1 [V0.D2], (R0)
+
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+
+		vld1 := arm64.AVLD1
+		p := s.Prog(vld1)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r1
+		p.To.Type = obj.TYPE_REGLIST
+		p.To.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		p1 := s.Prog(vld1)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = r2
+		p1.To.Type = obj.TYPE_REGLIST
+		p1.To.Offset = 1&31 | arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		vorr := arm64.AVORR
+		p2 := s.Prog(vorr)
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = arm64.REG_ARNG + (arm64.REG_V1 & 31) + ((arm64.ARNG_16B & 15) << 5)
+		p2.Reg = arm64.REG_ARNG + (arm64.REG_V0 & 31) + ((arm64.ARNG_16B & 15) << 5)
+		p2.To.Type = obj.TYPE_REG
+		p2.To.Reg = arm64.REG_ARNG + (arm64.REG_V0 & 31) + ((arm64.ARNG_16B & 15) << 5)
+
+		vst1 := arm64.AVST1
+		p3 := s.Prog(vst1)
+		p3.From.Type = obj.TYPE_REGLIST
+		p3.From.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+		p3.To.Type = obj.TYPE_MEM
+		p3.To.Reg = r0
+
+	case ssa.OpARM64LoweredSimdXor8x16:
+		// VLD1 (R1), [V0.D2]
+		// VLD1 (R2), [V1.D2]
+		// VEOR V1.B16 V0.B16 V0.B16
+		// VST1 [V0.D2], (R0)
+
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+
+		vld1 := arm64.AVLD1
+		p := s.Prog(vld1)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r1
+		p.To.Type = obj.TYPE_REGLIST
+		p.To.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		p1 := s.Prog(vld1)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = r2
+		p1.To.Type = obj.TYPE_REGLIST
+		p1.To.Offset = 1&31 | arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		veor := arm64.AVEOR
+		p2 := s.Prog(veor)
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = arm64.REG_ARNG + (arm64.REG_V1 & 31) + ((arm64.ARNG_16B & 15) << 5)
+		p2.Reg = arm64.REG_ARNG + (arm64.REG_V0 & 31) + ((arm64.ARNG_16B & 15) << 5)
+		p2.To.Type = obj.TYPE_REG
+		p2.To.Reg = arm64.REG_ARNG + (arm64.REG_V0 & 31) + ((arm64.ARNG_16B & 15) << 5)
+
+		vst1 := arm64.AVST1
+		p3 := s.Prog(vst1)
+		p3.From.Type = obj.TYPE_REGLIST
+		p3.From.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+		p3.To.Type = obj.TYPE_MEM
+		p3.To.Reg = r0
+
+	case ssa.OpARM64LoweredSimdReduceMax8x16:
+		// VLD1 (R0), [V0.D2]
+		// VMAX V0.B16, V1.B[0]
+		// VMOV V1.B[0], R1
+		// MOVB R1, ret+8(FP)
+
+		r0 := v.Args[0].Reg()
+		out := v.Reg0()
+
+		vld1 := arm64.AVLD1
+		p := s.Prog(vld1)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r0
+		p.To.Type = obj.TYPE_REGLIST
+		p.To.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		word := arm64.AWORD
+		p1 := s.Prog(word)
+		p1.To.Type = obj.TYPE_CONST
+		p1.To.Offset = 0x6e30a801 // VMAX V0.B16, V1.B[0]
+
+		vmov := arm64.AVMOV
+		p2 := s.Prog(vmov)
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = arm64.REG_ELEM + (arm64.REG_V1 & 31) + ((arm64.ARNG_B & 15) << 5)
+		p2.From.Index = 0
+		p2.To.Type = obj.TYPE_REG
+		p2.To.Reg = arm64.REG_R1
+
+		movb := arm64.AMOVB
+		p3 := s.Prog(movb)
+		p3.From.Type = obj.TYPE_REG
+		p3.From.Reg = arm64.REG_R1
+		p3.To.Type = obj.TYPE_REG
+		p3.To.Reg = out
+
+	case ssa.OpARM64LoweredSimdReduceMin8x16:
+		// VLD1 (R0), [V0.D2]
+		// VMIN V0.B16, V1.B[0]
+		// VMOV V1.B[0], R1
+		// MOVB R1, ret+8(FP)
+
+		r0 := v.Args[0].Reg()
+		out := v.Reg0()
+
+		vld1 := arm64.AVLD1
+		p := s.Prog(vld1)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r0
+		p.To.Type = obj.TYPE_REGLIST
+		p.To.Offset = /*0&31 |*/ arm64.ARNG_2D<<12 | 1<<30 | 3<<10 | 1<<60
+
+		word := arm64.AWORD
+		p1 := s.Prog(word)
+		p1.To.Type = obj.TYPE_CONST
+		p1.To.Offset = 0x6e31a801 // VMIN V0.B16, V1.B[0]
+
+		vmov := arm64.AVMOV
+		p2 := s.Prog(vmov)
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = arm64.REG_ELEM + (arm64.REG_V1 & 31) + ((arm64.ARNG_B & 15) << 5)
+		p2.From.Index = 0
+		p2.To.Type = obj.TYPE_REG
+		p2.To.Reg = arm64.REG_R1
+
+		movb := arm64.AMOVB
+		p3 := s.Prog(movb)
+		p3.From.Type = obj.TYPE_REG
+		p3.From.Reg = arm64.REG_R1
+		p3.To.Type = obj.TYPE_REG
+		p3.To.Reg = out
+
 	case ssa.OpARM64LoweredAtomicExchange64,
 		ssa.OpARM64LoweredAtomicExchange32:
 		// LDAXR	(Rarg0), Rout
diff --git a/src/cmd/compile/internal/ssa/_gen/ARM64.rules b/src/cmd/compile/internal/ssa/_gen/ARM64.rules
index c5ee0285d9..4f774eed2a 100644
--- a/src/cmd/compile/internal/ssa/_gen/ARM64.rules
+++ b/src/cmd/compile/internal/ssa/_gen/ARM64.rules
@@ -559,6 +559,26 @@
 
 (JumpTable idx) => (JUMPTABLE {makeJumpTableSym(b)} idx (MOVDaddr <typ.Uintptr> {makeJumpTableSym(b)} (SB)))
 
+// simd intrinsics
+(SimdAddU8x16 ...) => (LoweredSimdAdd8x16 ...)
+(SimdAdd8x16 ...) => (LoweredSimdAdd8x16 ...)
+(SimdSaturatingAddU8x16 ...) => (LoweredSimdSaturatingAddU8x16 ...)
+(SimdSaturatingAdd8x16 ...) => (LoweredSimdSaturatingAdd8x16 ...)
+(SimdSubU8x16 ...) => (LoweredSimdSub8x16 ...)
+(SimdSub8x16 ...) => (LoweredSimdSub8x16 ...)
+(SimdSaturatingSubU8x16 ...) => (LoweredSimdSaturatingSubU8x16 ...)
+(SimdSaturatingSub8x16 ...) => (LoweredSimdSaturatingSub8x16 ...)
+(SimdAndU8x16 ...) => (LoweredSimdAnd8x16 ...)
+(SimdAnd8x16 ...) => (LoweredSimdAnd8x16 ...)
+(SimdOrU8x16 ...) => (LoweredSimdOr8x16 ...)
+(SimdOr8x16 ...) => (LoweredSimdOr8x16 ...)
+(SimdXorU8x16 ...) => (LoweredSimdXor8x16 ...)
+(SimdXor8x16 ...) => (LoweredSimdXor8x16 ...)
+(SimdReduceMaxU8x16 ...) => (LoweredSimdReduceMax8x16 ...)
+(SimdReduceMax8x16 ...) => (LoweredSimdReduceMax8x16 ...)
+(SimdReduceMinU8x16 ...) => (LoweredSimdReduceMin8x16 ...)
+(SimdReduceMin8x16 ...) => (LoweredSimdReduceMin8x16 ...)
+
 // atomic intrinsics
 // Note: these ops do not accept offset.
 (AtomicLoad8   ...) => (LDARB ...)
diff --git a/src/cmd/compile/internal/ssa/_gen/ARM64Ops.go b/src/cmd/compile/internal/ssa/_gen/ARM64Ops.go
index 5a98aa0c54..0e35be4034 100644
--- a/src/cmd/compile/internal/ssa/_gen/ARM64Ops.go
+++ b/src/cmd/compile/internal/ssa/_gen/ARM64Ops.go
@@ -644,6 +644,48 @@ func init() {
 		{name: "STLR", argLength: 3, reg: gpstore, asm: "STLR", faultOnNilArg0: true, hasSideEffects: true},
 		{name: "STLRW", argLength: 3, reg: gpstore, asm: "STLRW", faultOnNilArg0: true, hasSideEffects: true},
 
+		// SIMD wrapping add u8x16.
+		// *arg0 = arg1 + arg2. arg3=mem. returns memory.
+		{name: "LoweredSimdAdd8x16", argLength: 4, reg: gpstore2, typ: "Mem", faultOnNilArg0: true, hasSideEffects: true},
+
+		// SIMD saturating add u8x16.
+		// *arg0 = arg1 + arg2. arg3=mem. returns memory.
+		{name: "LoweredSimdSaturatingAddU8x16", argLength: 4, reg: gpstore2, typ: "Mem", faultOnNilArg0: true, hasSideEffects: true},
+
+		// SIMD saturating add i8x16.
+		// *arg0 = arg1 + arg2. arg3=mem. returns memory.
+		{name: "LoweredSimdSaturatingAdd8x16", argLength: 4, reg: gpstore2, typ: "Mem", faultOnNilArg0: true, hasSideEffects: true},
+
+		// SIMD wrapping sub u8x16.
+		// *arg0 = arg1 - arg2. arg3=mem. returns memory.
+		{name: "LoweredSimdSub8x16", argLength: 4, reg: gpstore2, typ: "Mem", faultOnNilArg0: true, hasSideEffects: true},
+
+		// SIMD saturating sub u8x16.
+		// *arg0 = arg1 - arg2. arg3=mem. returns memory.
+		{name: "LoweredSimdSaturatingSubU8x16", argLength: 4, reg: gpstore2, typ: "Mem", faultOnNilArg0: true, hasSideEffects: true},
+
+		// SIMD saturating sub i8x16.
+		// *arg0 = arg1 - arg2. arg3=mem. returns memory.
+		{name: "LoweredSimdSaturatingSub8x16", argLength: 4, reg: gpstore2, typ: "Mem", faultOnNilArg0: true, hasSideEffects: true},
+
+		// SIMD and i8x16.
+		// *arg0 = arg1 & arg2. arg3=mem. returns memory.
+		{name: "LoweredSimdAnd8x16", argLength: 4, reg: gpstore2, typ: "Mem", faultOnNilArg0: true, hasSideEffects: true},
+
+		// SIMD or i8x16.
+		// *arg0 = arg1 | arg2. arg3=mem. returns memory.
+		{name: "LoweredSimdOr8x16", argLength: 4, reg: gpstore2, typ: "Mem", faultOnNilArg0: true, hasSideEffects: true},
+
+		// SIMD xor i8x16.
+		// *arg0 = arg1 ^ arg2. arg3=mem. returns memory.
+		{name: "LoweredSimdXor8x16", argLength: 4, reg: gpstore2, typ: "Mem", faultOnNilArg0: true, hasSideEffects: true},
+
+		// SIMD max i8x16.
+		{name: "LoweredSimdReduceMax8x16", argLength: 2, reg: gpstore2, typ: "(UInt8,Mem)", faultOnNilArg0: true},
+
+		// SIMD min i8x16.
+		{name: "LoweredSimdReduceMin8x16", argLength: 2, reg: gpstore2, typ: "(UInt8,Mem)", faultOnNilArg0: true},
+
 		// atomic exchange.
 		// store arg1 to arg0. arg2=mem. returns <old content of *arg0, memory>. auxint must be zero.
 		// LDAXR	(Rarg0), Rout
diff --git a/src/cmd/compile/internal/ssa/_gen/genericOps.go b/src/cmd/compile/internal/ssa/_gen/genericOps.go
index 69eb48ce44..62757a54fb 100644
--- a/src/cmd/compile/internal/ssa/_gen/genericOps.go
+++ b/src/cmd/compile/internal/ssa/_gen/genericOps.go
@@ -586,6 +586,24 @@ var genericOps = []opData{
 	{name: "SelectNAddr", argLength: 1, aux: "Int64"}, // arg0=result, auxint=field index.  Returns the address of auxint'th member. Used for un-SSA-able result types.
 	{name: "MakeResult", argLength: -1},               // arg0 .. are components of a "Result" (like the result from a Call). The last arg should be memory (like the result from a call).
 
+	// SIMD
+	{name: "SimdAddU8x16", argLength: 4, typ: "Mem", hasSideEffects: true},            // *arg0 = arg1 + arg2.  arg3=memory.  Returns memory.
+	{name: "SimdAdd8x16", argLength: 4, typ: "Mem", hasSideEffects: true},             // *arg0 = arg1 + arg2.  arg3=memory.  Returns memory.
+	{name: "SimdStaturatingAddU8x16", argLength: 4, typ: "Mem", hasSideEffects: true}, // *arg0 = arg1 + arg2.  arg3=memory.  Returns memory.
+	{name: "SimdStaturatingAdd8x16", argLength: 4, typ: "Mem", hasSideEffects: true},  // *arg0 = arg1 + arg2.  arg3=memory.  Returns memory.
+	{name: "SimdSubU8x16", argLength: 4, typ: "Mem", hasSideEffects: true},            // *arg0 = arg1 - arg2.  arg3=memory.  Returns memory.
+	{name: "SimdSub8x16", argLength: 4, typ: "Mem", hasSideEffects: true},             // *arg0 = arg1 - arg2.  arg3=memory.  Returns memory.
+	{name: "SimdStaturatingSubU8x16", argLength: 4, typ: "Mem", hasSideEffects: true}, // *arg0 = arg1 - arg2.  arg3=memory.  Returns memory.
+	{name: "SimdStaturatingSub8x16", argLength: 4, typ: "Mem", hasSideEffects: true},  // *arg0 = arg1 - arg2.  arg3=memory.  Returns memory.
+	{name: "SimdAndU8x16", argLength: 4, typ: "Mem", hasSideEffects: true},            // *arg0 = arg1 & arg2.  arg3=memory.  Returns memory.
+	{name: "SimdAnd8x16", argLength: 4, typ: "Mem", hasSideEffects: true},             // *arg0 = arg1 & arg2.  arg3=memory.  Returns memory.
+	{name: "SimdOrU8x16", argLength: 4, typ: "Mem", hasSideEffects: true},             // *arg0 = arg1 | arg2.  arg3=memory.  Returns memory.
+	{name: "SimdOr8x16", argLength: 4, typ: "Mem", hasSideEffects: true},              // *arg0 = arg1 | arg2.  arg3=memory.  Returns memory.
+	{name: "SimdXorU8x16", argLength: 4, typ: "Mem", hasSideEffects: true},            // *arg0 = arg1 ^ arg2.  arg3=memory.  Returns memory.
+	{name: "SimdXor8x16", argLength: 4, typ: "Mem", hasSideEffects: true},             // *arg0 = arg1 ^ arg2.  arg3=memory.  Returns memory.
+	{name: "SimdReduceMax8x16", argLength: 2, typ: "(UInt8,Mem)"},
+	{name: "SimdReduceMin8x16", argLength: 2, typ: "(UInt8,Mem)"},
+
 	// Atomic operations used for semantically inlining sync/atomic and
 	// runtime/internal/atomic. Atomic loads return a new memory so that
 	// the loads are properly ordered with respect to other loads and
diff --git a/src/cmd/compile/internal/ssa/opGen.go b/src/cmd/compile/internal/ssa/opGen.go
index c552832520..30f999eefc 100644
--- a/src/cmd/compile/internal/ssa/opGen.go
+++ b/src/cmd/compile/internal/ssa/opGen.go
@@ -1704,6 +1704,17 @@ const (
 	OpARM64STLRB
 	OpARM64STLR
 	OpARM64STLRW
+	OpARM64LoweredSimdAdd8x16
+	OpARM64LoweredSimdSaturatingAdd8x16
+	OpARM64LoweredSimdSaturatingAddU8x16
+	OpARM64LoweredSimdSub8x16
+	OpARM64LoweredSimdSaturatingSub8x16
+	OpARM64LoweredSimdSaturatingSubU8x16
+  OpARM64LoweredSimdAnd8x16
+  OpARM64LoweredSimdOr8x16
+  OpARM64LoweredSimdXor8x16
+  OpARM64LoweredSimdReduceMax8x16
+  OpARM64LoweredSimdReduceMin8x16
 	OpARM64LoweredAtomicExchange64
 	OpARM64LoweredAtomicExchange32
 	OpARM64LoweredAtomicExchange64Variant
@@ -3189,6 +3200,24 @@ const (
 	OpSelectN
 	OpSelectNAddr
 	OpMakeResult
+	OpSimdAddU8x16
+	OpSimdAdd8x16
+	OpSimdSaturatingAddU8x16
+	OpSimdSaturatingAdd8x16
+	OpSimdSubU8x16
+	OpSimdSub8x16
+	OpSimdSaturatingSubU8x16
+	OpSimdSaturatingSub8x16
+  OpSimdAndU8x16
+	OpSimdAnd8x16
+  OpSimdOrU8x16
+	OpSimdOr8x16
+  OpSimdXorU8x16
+	OpSimdXor8x16
+  OpSimdReduceMaxU8x16
+	OpSimdReduceMax8x16
+  OpSimdReduceMinU8x16
+	OpSimdReduceMin8x16
 	OpAtomicLoad8
 	OpAtomicLoad32
 	OpAtomicLoad64
@@ -22771,6 +22800,149 @@ var opcodeTable = [...]opInfo{
 			},
 		},
 	},
+	{
+		name:           "LoweredSimdAdd8x16",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{2, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{0, 9223372038733561855}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30 SP SB
+			},
+		},
+	},
+	{
+		name:           "LoweredSimdStaturatingAdd8x16",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{2, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{0, 9223372038733561855}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30 SP SB
+			},
+		},
+	},
+	{
+		name:           "LoweredSimdStaturatingAddU8x16",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{2, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{0, 9223372038733561855}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30 SP SB
+			},
+		},
+	},
+	{
+		name:           "LoweredSimdSub8x16",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{2, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{0, 9223372038733561855}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30 SP SB
+			},
+		},
+	},
+	{
+		name:           "LoweredSimdStaturatingSub8x16",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{2, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{0, 9223372038733561855}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30 SP SB
+			},
+		},
+	},
+	{
+		name:           "LoweredSimdStaturatingSubU8x16",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{2, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{0, 9223372038733561855}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30 SP SB
+			},
+		},
+	},
+  {
+		name:           "LoweredSimdAnd8x16",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{2, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{0, 9223372038733561855}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30 SP SB
+			},
+		},
+	},
+  {
+		name:           "LoweredSimdOr8x16",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{2, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{0, 9223372038733561855}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30 SP SB
+			},
+		},
+	},
+  {
+		name:           "LoweredSimdXor8x16",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{2, 805044223},           // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30
+				{0, 9223372038733561855}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30 SP SB
+			},
+		},
+	},
+  {
+		name:           "LoweredSimdReduceMax8x16",
+		argLen:         2,
+		faultOnNilArg0: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 9223372038733561855}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30 SP SB
+			},
+      outputs: []outputInfo{
+				{0, 670826495}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 R30
+			},
+		},
+	},
+  {
+		name:           "LoweredSimdReduceMin8x16",
+		argLen:         2,
+		faultOnNilArg0: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{0, 9223372038733561855}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 g R30 SP SB
+			},
+      outputs: []outputInfo{
+				{0, 670826495}, // R0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16 R17 R19 R20 R21 R22 R23 R24 R25 R26 R30
+			},
+		},
+	},
 	{
 		name:            "LoweredAtomicExchange64",
 		argLen:          3,
@@ -40269,6 +40441,110 @@ var opcodeTable = [...]opInfo{
 		argLen:  -1,
 		generic: true,
 	},
+	{
+		name:           "SimdAddU8x16",
+		argLen:         4,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "SimdAdd8x16",
+		argLen:         4,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "SimdSaturatingAddU8x16",
+		argLen:         4,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "SimdSaturatingAdd8x16",
+		argLen:         4,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "SimdSubU8x16",
+		argLen:         4,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "SimdSub8x16",
+		argLen:         4,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "SimdSaturatingSubU8x16",
+		argLen:         4,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "SimdSaturatingSub8x16",
+		argLen:         4,
+		hasSideEffects: true,
+		generic:        true,
+	},
+  {
+		name:           "SimdAndU8x16",
+		argLen:         4,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "SimdAnd8x16",
+		argLen:         4,
+		hasSideEffects: true,
+		generic:        true,
+	},
+  {
+		name:           "SimdOrU8x16",
+		argLen:         4,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "SimdOr8x16",
+		argLen:         4,
+		hasSideEffects: true,
+		generic:        true,
+	},
+  {
+		name:           "SimdXorU8x16",
+		argLen:         4,
+		hasSideEffects: true,
+		generic:        true,
+	},
+	{
+		name:           "SimdXor8x16",
+		argLen:         4,
+		hasSideEffects: true,
+		generic:        true,
+	},
+  {
+		name:           "SimdReduceMaxU8x16",
+		argLen:         2,
+		generic:        true,
+	},
+  {
+		name:           "SimdReduceMax8x16",
+		argLen:         2,
+		generic:        true,
+	},
+    {
+		name:           "SimdReduceMinU8x16",
+		argLen:         2,
+		generic:        true,
+	},
+  {
+		name:           "SimdReduceMin8x16",
+		argLen:         2,
+		generic:        true,
+	},
 	{
 		name:    "AtomicLoad8",
 		argLen:  2,
diff --git a/src/cmd/compile/internal/ssa/rewriteARM64.go b/src/cmd/compile/internal/ssa/rewriteARM64.go
index f0a4425502..0e6732f4f7 100644
--- a/src/cmd/compile/internal/ssa/rewriteARM64.go
+++ b/src/cmd/compile/internal/ssa/rewriteARM64.go
@@ -1049,6 +1049,60 @@ func rewriteValueARM64(v *Value) bool {
 	case OpSignExt8to64:
 		v.Op = OpARM64MOVBreg
 		return true
+	case OpSimdAdd8x16:
+		v.Op = OpARM64LoweredSimdAdd8x16
+		return true
+	case OpSimdAddU8x16:
+		v.Op = OpARM64LoweredSimdAdd8x16
+		return true
+	case OpSimdSaturatingAdd8x16:
+		v.Op = OpARM64LoweredSimdSaturatingAdd8x16
+		return true
+	case OpSimdSaturatingAddU8x16:
+		v.Op = OpARM64LoweredSimdSaturatingAddU8x16
+		return true
+	case OpSimdSub8x16:
+		v.Op = OpARM64LoweredSimdSub8x16
+		return true
+	case OpSimdSubU8x16:
+		v.Op = OpARM64LoweredSimdSub8x16
+		return true
+	case OpSimdSaturatingSub8x16:
+		v.Op = OpARM64LoweredSimdSaturatingSub8x16
+		return true
+	case OpSimdSaturatingSubU8x16:
+		v.Op = OpARM64LoweredSimdSaturatingSubU8x16
+		return true
+  case OpSimdAnd8x16:
+		v.Op = OpARM64LoweredSimdAnd8x16
+		return true
+	case OpSimdAndU8x16:
+		v.Op = OpARM64LoweredSimdAnd8x16
+		return true
+  case OpSimdOr8x16:
+		v.Op = OpARM64LoweredSimdOr8x16
+		return true
+	case OpSimdOrU8x16:
+		v.Op = OpARM64LoweredSimdOr8x16
+		return true
+  case OpSimdXor8x16:
+		v.Op = OpARM64LoweredSimdXor8x16
+		return true
+	case OpSimdXorU8x16:
+		v.Op = OpARM64LoweredSimdXor8x16
+		return true
+  case OpSimdReduceMax8x16:
+		v.Op = OpARM64LoweredSimdReduceMax8x16
+		return true
+	case OpSimdReduceMaxU8x16:
+		v.Op = OpARM64LoweredSimdReduceMax8x16
+		return true
+  case OpSimdReduceMin8x16:
+		v.Op = OpARM64LoweredSimdReduceMin8x16
+		return true
+	case OpSimdReduceMinU8x16:
+		v.Op = OpARM64LoweredSimdReduceMin8x16
+		return true
 	case OpSlicemask:
 		return rewriteValueARM64_OpSlicemask(v)
 	case OpSqrt:
diff --git a/src/cmd/compile/internal/ssagen/ssa.go b/src/cmd/compile/internal/ssagen/ssa.go
index c794d6ffd9..e4b9ececcc 100644
--- a/src/cmd/compile/internal/ssagen/ssa.go
+++ b/src/cmd/compile/internal/ssagen/ssa.go
@@ -4287,6 +4287,76 @@ func InitTables() {
 	addF("runtime/internal/sys", "PrefetchStreamed", makePrefetchFunc(ssa.OpPrefetchCacheStreamed),
 		sys.AMD64, sys.ARM64, sys.PPC64)
 
+	/******** runtime/internal/simd ********/
+	addF("runtime/internal/simd", "Add8x16",
+		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
+			s.vars[memVar] = s.newValue4(ssa.OpSimdAdd8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
+			return nil
+		},
+		sys.ARM64)
+	addF("runtime/internal/simd", "SaturatingAdd8x16",
+		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
+			s.vars[memVar] = s.newValue4(ssa.OpSimdSaturatingAdd8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
+			return nil
+		},
+		sys.ARM64)
+	addF("runtime/internal/simd", "SaturatingAddU8x16",
+		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
+			s.vars[memVar] = s.newValue4(ssa.OpSimdSaturatingAddU8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
+			return nil
+		},
+		sys.ARM64)
+	addF("runtime/internal/simd", "Sub8x16",
+		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
+			s.vars[memVar] = s.newValue4(ssa.OpSimdSub8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
+			return nil
+		},
+		sys.ARM64)
+	addF("runtime/internal/simd", "SaturatingSub8x16",
+		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
+			s.vars[memVar] = s.newValue4(ssa.OpSimdSaturatingSub8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
+			return nil
+		},
+		sys.ARM64)
+	addF("runtime/internal/simd", "SaturatingSubU8x16",
+		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
+			s.vars[memVar] = s.newValue4(ssa.OpSimdSaturatingSubU8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
+			return nil
+		},
+		sys.ARM64)
+	addF("runtime/internal/simd", "And8x16",
+		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
+			s.vars[memVar] = s.newValue4(ssa.OpSimdAnd8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
+			return nil
+		},
+		sys.ARM64)
+	addF("runtime/internal/simd", "Or8x16",
+		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
+			s.vars[memVar] = s.newValue4(ssa.OpSimdOr8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
+			return nil
+		},
+		sys.ARM64)
+	addF("runtime/internal/simd", "Xor8x16",
+		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
+			s.vars[memVar] = s.newValue4(ssa.OpSimdXor8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
+			return nil
+		},
+		sys.ARM64)
+	addF("runtime/internal/simd", "ReduceMax8x16",
+		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
+			v := s.newValue2(ssa.OpSimdReduceMax8x16, types.NewTuple(types.Types[types.TUINT32], types.TypeMem), args[0], s.mem())
+			s.vars[memVar] = s.newValue1(ssa.OpSelect1, types.TypeMem, v)
+			return s.newValue1(ssa.OpSelect0, types.Types[types.TUINT32], v)
+		},
+		sys.ARM64)
+	addF("runtime/internal/simd", "ReduceMin8x16",
+		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
+			v := s.newValue2(ssa.OpSimdReduceMin8x16, types.NewTuple(types.Types[types.TUINT32], types.TypeMem), args[0], s.mem())
+			s.vars[memVar] = s.newValue1(ssa.OpSelect1, types.TypeMem, v)
+			return s.newValue1(ssa.OpSelect0, types.Types[types.TUINT32], v)
+		},
+		sys.ARM64)
+
 	/******** runtime/internal/atomic ********/
 	addF("runtime/internal/atomic", "Load",
 		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
@@ -4386,7 +4456,6 @@ func InitTables() {
 	type atomicOpEmitter func(s *state, n *ir.CallExpr, args []*ssa.Value, op ssa.Op, typ types.Kind)
 
 	makeAtomicGuardedIntrinsicARM64 := func(op0, op1 ssa.Op, typ, rtyp types.Kind, emit atomicOpEmitter) intrinsicBuilder {
-
 		return func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
 			// Target Atomic feature is identified by dynamic detection
 			addr := s.entryNewValue1A(ssa.OpAddr, types.Types[types.TBOOL].PtrTo(), ir.Syms.ARM64HasATOMICS, s.sb)
@@ -5015,6 +5084,26 @@ func InitTables() {
 	alias("runtime/internal/sys", "Len64", "math/bits", "Len64", all...)
 	alias("runtime/internal/sys", "OnesCount64", "math/bits", "OnesCount64", all...)
 
+	/******** simd ********/
+	alias("simd", "addU8x16", "runtime/internal/simd", "Add8x16", sys.ArchARM64)
+	alias("simd", "add8x16", "runtime/internal/simd", "Add8x16", sys.ArchARM64)
+	alias("simd", "saturatingAddU8x16", "runtime/internal/simd", "SaturatingAddU8x16", sys.ArchARM64)
+	alias("simd", "saturatingAdd8x16", "runtime/internal/simd", "SaturatingAdd8x16", sys.ArchARM64)
+	alias("simd", "subU8x16", "runtime/internal/simd", "Sub8x16", sys.ArchARM64)
+	alias("simd", "sub8x16", "runtime/internal/simd", "Sub8x16", sys.ArchARM64)
+	alias("simd", "saturatingSubU8x16", "runtime/internal/simd", "SaturatingSubU8x16", sys.ArchARM64)
+	alias("simd", "saturatingSub8x16", "runtime/internal/simd", "SaturatingSub8x16", sys.ArchARM64)
+	alias("simd", "andU8x16", "runtime/internal/simd", "And8x16", sys.ArchARM64)
+	alias("simd", "and8x16", "runtime/internal/simd", "And8x16", sys.ArchARM64)
+	alias("simd", "orU8x16", "runtime/internal/simd", "Or8x16", sys.ArchARM64)
+	alias("simd", "or8x16", "runtime/internal/simd", "Or8x16", sys.ArchARM64)
+	alias("simd", "xorU8x16", "runtime/internal/simd", "Xor8x16", sys.ArchARM64)
+	alias("simd", "xor8x16", "runtime/internal/simd", "Xor8x16", sys.ArchARM64)
+	alias("simd", "ReduceMaxU8x16", "runtime/internal/simd", "ReduceMax8x16", sys.ArchARM64)
+	alias("simd", "ReduceMax8x16", "runtime/internal/simd", "ReduceMax8x16", sys.ArchARM64)
+	alias("simd", "ReduceMinU8x16", "runtime/internal/simd", "ReduceMin8x16", sys.ArchARM64)
+	alias("simd", "ReduceMin8x16", "runtime/internal/simd", "ReduceMin8x16", sys.ArchARM64)
+
 	/******** sync/atomic ********/
 
 	// Note: these are disabled by flag_race in findIntrinsic below.
