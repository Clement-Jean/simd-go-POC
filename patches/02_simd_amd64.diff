diff --git a/src/cmd/compile/internal/amd64/ssa.go b/src/cmd/compile/internal/amd64/ssa.go
index ab762c24f6..cc1570cc41 100644
--- a/src/cmd/compile/internal/amd64/ssa.go
+++ b/src/cmd/compile/internal/amd64/ssa.go
@@ -1250,6 +1250,241 @@ func ssaGenValue(s *ssagen.State, v *ssa.Value) {
 		if base.Debug.Nil != 0 && v.Pos.Line() > 1 { // v.Pos.Line()==1 in generated wrappers
 			base.WarnfAt(v.Pos, "generated nil check")
 		}
+
+	case ssa.OpAMD64LoweredSimdAdd8x16:
+		// VMOVDQA (arg1), X0
+		// VMOVDQA (arg2), X1
+		// VPADDB X1, X0, X0
+		// VMOVDQA X0, (arg0)
+
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+
+		vmovdqa := x86.AVMOVDQA
+		p := s.Prog(vmovdqa)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = x86.REG_X0
+
+		p1 := s.Prog(vmovdqa)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = r2
+		p1.To.Type = obj.TYPE_REG
+		p1.To.Reg = x86.REG_X1
+
+		vpaddb := x86.AVPADDB
+		p2 := s.Prog(vpaddb)
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = x86.REG_X0
+		p2.RestArgs = []obj.AddrPos{
+			{Addr: obj.Addr{Type: obj.TYPE_REG, Reg: x86.REG_X1}},
+		}
+		p2.To.Type = obj.TYPE_REG
+		p2.To.Reg = x86.REG_X0
+
+		p3 := s.Prog(vmovdqa)
+		p3.From.Type = obj.TYPE_REG
+		p3.From.Reg = x86.REG_X0
+		p3.To.Type = obj.TYPE_MEM
+		p3.To.Reg = r0
+
+	case ssa.OpAMD64LoweredSimdSub8x16:
+		// VMOVDQA (arg1), X0
+		// VMOVDQA (arg2), X1
+		// VPSUBB X1, X0, X0
+		// VMOVDQA X0, (arg0)
+
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+
+		vmovdqa := x86.AVMOVDQA
+		p := s.Prog(vmovdqa)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = x86.REG_X0
+
+		p1 := s.Prog(vmovdqa)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = r2
+		p1.To.Type = obj.TYPE_REG
+		p1.To.Reg = x86.REG_X1
+
+		vpsubb := x86.AVPSUBB
+		p2 := s.Prog(vpsubb)
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = x86.REG_X0
+		p2.RestArgs = []obj.AddrPos{
+			{Addr: obj.Addr{Type: obj.TYPE_REG, Reg: x86.REG_X1}},
+		}
+		p2.To.Type = obj.TYPE_REG
+		p2.To.Reg = x86.REG_X0
+
+		p3 := s.Prog(vmovdqa)
+		p3.From.Type = obj.TYPE_REG
+		p3.From.Reg = x86.REG_X0
+		p3.To.Type = obj.TYPE_MEM
+		p3.To.Reg = r0
+
+	case ssa.OpAMD64LoweredSimdSaturatingAdd8x16:
+		// VMOVDQA (arg1), X0
+		// VMOVDQA (arg2), X1
+		// VPADDSB X1, X0, X0
+		// VMOVDQA X0, (arg0)
+
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+
+		vmovdqa := x86.AVMOVDQA
+		p := s.Prog(vmovdqa)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = x86.REG_X0
+
+		p1 := s.Prog(vmovdqa)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = r2
+		p1.To.Type = obj.TYPE_REG
+		p1.To.Reg = x86.REG_X1
+
+		vpaddsb := x86.AVPADDSB
+		p2 := s.Prog(vpaddsb)
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = x86.REG_X0
+		p2.RestArgs = []obj.AddrPos{
+			{Addr: obj.Addr{Type: obj.TYPE_REG, Reg: x86.REG_X1}},
+		}
+		p2.To.Type = obj.TYPE_REG
+		p2.To.Reg = x86.REG_X0
+
+		p3 := s.Prog(vmovdqa)
+		p3.From.Type = obj.TYPE_REG
+		p3.From.Reg = x86.REG_X0
+		p3.To.Type = obj.TYPE_MEM
+		p3.To.Reg = r0
+
+	case ssa.OpAMD64LoweredSimdSaturatingAddU8x16:
+		// VMOVDQA (arg1), X0
+		// VMOVDQA (arg2), X1
+		// VPADDUSB X1, X0, X0
+		// VMOVDQA X0, (arg0)
+
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+
+		vmovdqa := x86.AVMOVDQA
+		p := s.Prog(vmovdqa)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = x86.REG_X0
+
+		p1 := s.Prog(vmovdqa)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = r2
+		p1.To.Type = obj.TYPE_REG
+		p1.To.Reg = x86.REG_X1
+
+		vpaddusb := x86.AVPADDUSB
+		p2 := s.Prog(vpaddusb)
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = x86.REG_X0
+		p2.RestArgs = []obj.AddrPos{
+			{Addr: obj.Addr{Type: obj.TYPE_REG, Reg: x86.REG_X1}},
+		}
+		p2.To.Type = obj.TYPE_REG
+		p2.To.Reg = x86.REG_X0
+
+		p3 := s.Prog(vmovdqa)
+		p3.From.Type = obj.TYPE_REG
+		p3.From.Reg = x86.REG_X0
+		p3.To.Type = obj.TYPE_MEM
+		p3.To.Reg = r0
+
+	case ssa.OpAMD64LoweredSimdSaturatingSub8x16:
+		// VMOVDQA (arg1), X0
+		// VMOVDQA (arg2), X1
+		// VPSUBSB X1, X0, X0
+		// VMOVDQA X0, (arg0)
+
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+
+		vmovdqa := x86.AVMOVDQA
+		p := s.Prog(vmovdqa)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = x86.REG_X0
+
+		p1 := s.Prog(vmovdqa)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = r2
+		p1.To.Type = obj.TYPE_REG
+		p1.To.Reg = x86.REG_X1
+
+		vpsubsb := x86.AVPSUBSB
+		p2 := s.Prog(vpsubsb)
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = x86.REG_X0
+		p2.RestArgs = []obj.AddrPos{
+			{Addr: obj.Addr{Type: obj.TYPE_REG, Reg: x86.REG_X1}},
+		}
+		p2.To.Type = obj.TYPE_REG
+		p2.To.Reg = x86.REG_X0
+
+		p3 := s.Prog(vmovdqa)
+		p3.From.Type = obj.TYPE_REG
+		p3.From.Reg = x86.REG_X0
+		p3.To.Type = obj.TYPE_MEM
+		p3.To.Reg = r0
+
+	case ssa.OpAMD64LoweredSimdSaturatingSubU8x16:
+		// VMOVDQA (arg1), X0
+		// VMOVDQA (arg2), X1
+		// VPSUBUSB X1, X0, X0
+		// VMOVDQA X0, (arg0)
+
+		r0 := v.Args[0].Reg()
+		r1 := v.Args[1].Reg()
+		r2 := v.Args[2].Reg()
+
+		vmovdqa := x86.AVMOVDQA
+		p := s.Prog(vmovdqa)
+		p.From.Type = obj.TYPE_MEM
+		p.From.Reg = r1
+		p.To.Type = obj.TYPE_REG
+		p.To.Reg = x86.REG_X0
+
+		p1 := s.Prog(vmovdqa)
+		p1.From.Type = obj.TYPE_MEM
+		p1.From.Reg = r2
+		p1.To.Type = obj.TYPE_REG
+		p1.To.Reg = x86.REG_X1
+
+		vpsubusb := x86.AVPSUBUSB
+		p2 := s.Prog(vpsubusb)
+		p2.From.Type = obj.TYPE_REG
+		p2.From.Reg = x86.REG_X0
+		p2.RestArgs = []obj.AddrPos{
+			{Addr: obj.Addr{Type: obj.TYPE_REG, Reg: x86.REG_X1}},
+		}
+		p2.To.Type = obj.TYPE_REG
+		p2.To.Reg = x86.REG_X0
+
+		p3 := s.Prog(vmovdqa)
+		p3.From.Type = obj.TYPE_REG
+		p3.From.Reg = x86.REG_X0
+		p3.To.Type = obj.TYPE_MEM
+		p3.To.Reg = r0
+
 	case ssa.OpAMD64MOVBatomicload, ssa.OpAMD64MOVLatomicload, ssa.OpAMD64MOVQatomicload:
 		p := s.Prog(v.Op.Asm())
 		p.From.Type = obj.TYPE_MEM
diff --git a/src/cmd/compile/internal/ssa/_gen/AMD64.rules b/src/cmd/compile/internal/ssa/_gen/AMD64.rules
index 2a4c59ebfc..2f0cbe6c78 100644
--- a/src/cmd/compile/internal/ssa/_gen/AMD64.rules
+++ b/src/cmd/compile/internal/ssa/_gen/AMD64.rules
@@ -549,6 +549,16 @@
 
 (JumpTable idx) => (JUMPTABLE {makeJumpTableSym(b)} idx (LEAQ <typ.Uintptr> {makeJumpTableSym(b)} (SB)))
 
+// simd intrinsics
+(SimdAddU8x16 ...) => (LoweredSimdAdd8x16 ...)
+(SimdAdd8x16 ...) => (LoweredSimdAdd8x16 ...)
+(SimdSaturatingAddU8x16 ...) => (LoweredSimdSaturatingAddU8x16 ...)
+(SimdSaturatingAdd8x16 ...) => (LoweredSimdSaturatingAdd8x16 ...)
+(SimdSubU8x16 ...) => (LoweredSimdSub8x16 ...)
+(SimdSub8x16 ...) => (LoweredSimdSub8x16 ...)
+(SimdSaturatingSubU8x16 ...) => (LoweredSimdSaturatingSubU8x16 ...)
+(SimdSaturatingSub8x16 ...) => (LoweredSimdSaturatingSub8x16 ...)
+
 // Atomic loads.  Other than preserving their ordering with respect to other loads, nothing special here.
 (AtomicLoad8 ptr mem) => (MOVBatomicload ptr mem)
 (AtomicLoad32 ptr mem) => (MOVLatomicload ptr mem)
diff --git a/src/cmd/compile/internal/ssa/_gen/AMD64Ops.go b/src/cmd/compile/internal/ssa/_gen/AMD64Ops.go
index 606171947b..405c075d71 100644
--- a/src/cmd/compile/internal/ssa/_gen/AMD64Ops.go
+++ b/src/cmd/compile/internal/ssa/_gen/AMD64Ops.go
@@ -981,6 +981,30 @@ func init() {
 		{name: "LoweredPanicBoundsB", argLength: 3, aux: "Int64", reg: regInfo{inputs: []regMask{cx, dx}}, typ: "Mem", call: true}, // arg0=idx, arg1=len, arg2=mem, returns memory. AuxInt contains report code (see PanicBounds in generic.go).
 		{name: "LoweredPanicBoundsC", argLength: 3, aux: "Int64", reg: regInfo{inputs: []regMask{ax, cx}}, typ: "Mem", call: true}, // arg0=idx, arg1=len, arg2=mem, returns memory. AuxInt contains report code (see PanicBounds in generic.go).
 
+		// SIMD wrapping add u8x16.
+		// *arg0 = arg1 + arg2. arg3=mem. returns memory.
+		{name: "LoweredSimdAdd8x16", argLength: 4, reg: regInfo{inputs: []regMask{gpspsbg, gp, gp}}, typ: "Mem", faultOnNilArg0: true, hasSideEffects: true},
+
+		// SIMD saturating add u8x16.
+		// *arg0 = arg1 + arg2. arg3=mem. returns memory.
+		{name: "LoweredSimdSaturatingAddU8x16", argLength: 4, reg: regInfo{inputs: []regMask{gpspsbg, gp, gp}}, typ: "Mem", faultOnNilArg0: true, hasSideEffects: true},
+
+		// SIMD saturating add i8x16.
+		// *arg0 = arg1 + arg2. arg3=mem. returns memory.
+		{name: "LoweredSimdSaturatingAdd8x16", argLength: 4, reg: regInfo{inputs: []regMask{gpspsbg, gp, gp}}, typ: "Mem", faultOnNilArg0: true, hasSideEffects: true},
+
+		// SIMD wrapping sub u8x16.
+		// *arg0 = arg1 - arg2. arg3=mem. returns memory.
+		{name: "LoweredSimdSub8x16", argLength: 4, reg: regInfo{inputs: []regMask{gpspsbg, gp, gp}}, typ: "Mem", faultOnNilArg0: true, hasSideEffects: true},
+
+		// SIMD saturating sub u8x16.
+		// *arg0 = arg1 - arg2. arg3=mem. returns memory.
+		{name: "LoweredSimdSaturatingSubU8x16", argLength: 4, reg: regInfo{inputs: []regMask{gpspsbg, gp, gp}}, typ: "Mem", faultOnNilArg0: true, hasSideEffects: true},
+
+		// SIMD saturating sub i8x16.
+		// *arg0 = arg1 - arg2. arg3=mem. returns memory.
+		{name: "LoweredSimdSaturatingSub8x16", argLength: 4, reg: regInfo{inputs: []regMask{gpspsbg, gp, gp}}, typ: "Mem", faultOnNilArg0: true, hasSideEffects: true},
+
 		// Constant flag values. For any comparison, there are 5 possible
 		// outcomes: the three from the signed total order (<,==,>) and the
 		// three from the unsigned total order. The == cases overlap.
diff --git a/src/cmd/compile/internal/ssa/opGen.go b/src/cmd/compile/internal/ssa/opGen.go
index 88326aa8b4..1234637cf8 100644
--- a/src/cmd/compile/internal/ssa/opGen.go
+++ b/src/cmd/compile/internal/ssa/opGen.go
@@ -1052,6 +1052,12 @@ const (
 	OpAMD64LoweredPanicBoundsA
 	OpAMD64LoweredPanicBoundsB
 	OpAMD64LoweredPanicBoundsC
+	OpAMD64LoweredSimdAdd8x16
+	OpAMD64LoweredSimdSaturatingAddU8x16
+	OpAMD64LoweredSimdSaturatingAdd8x16
+	OpAMD64LoweredSimdSub8x16
+	OpAMD64LoweredSimdSaturatingSubU8x16
+	OpAMD64LoweredSimdSaturatingSub8x16
 	OpAMD64FlagEQ
 	OpAMD64FlagLT_ULT
 	OpAMD64FlagLT_UGT
@@ -13882,6 +13888,84 @@ var opcodeTable = [...]opInfo{
 			},
 		},
 	},
+	{
+		name:           "LoweredSimdAdd8x16",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 49135},      // AX CX DX BX BP SI DI R8 R9 R10 R11 R12 R13 R15
+				{2, 49135},      // AX CX DX BX BP SI DI R8 R9 R10 R11 R12 R13 R15
+				{0, 4295032831}, // AX CX DX BX SP BP SI DI R8 R9 R10 R11 R12 R13 g R15 SB
+			},
+		},
+	},
+	{
+		name:           "LoweredSimdSaturatingAddU8x16",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 49135},      // AX CX DX BX BP SI DI R8 R9 R10 R11 R12 R13 R15
+				{2, 49135},      // AX CX DX BX BP SI DI R8 R9 R10 R11 R12 R13 R15
+				{0, 4295032831}, // AX CX DX BX SP BP SI DI R8 R9 R10 R11 R12 R13 g R15 SB
+			},
+		},
+	},
+	{
+		name:           "LoweredSimdSaturatingAdd8x16",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 49135},      // AX CX DX BX BP SI DI R8 R9 R10 R11 R12 R13 R15
+				{2, 49135},      // AX CX DX BX BP SI DI R8 R9 R10 R11 R12 R13 R15
+				{0, 4295032831}, // AX CX DX BX SP BP SI DI R8 R9 R10 R11 R12 R13 g R15 SB
+			},
+		},
+	},
+	{
+		name:           "LoweredSimdSub8x16",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 49135},      // AX CX DX BX BP SI DI R8 R9 R10 R11 R12 R13 R15
+				{2, 49135},      // AX CX DX BX BP SI DI R8 R9 R10 R11 R12 R13 R15
+				{0, 4295032831}, // AX CX DX BX SP BP SI DI R8 R9 R10 R11 R12 R13 g R15 SB
+			},
+		},
+	},
+	{
+		name:           "LoweredSimdSaturatingSubU8x16",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 49135},      // AX CX DX BX BP SI DI R8 R9 R10 R11 R12 R13 R15
+				{2, 49135},      // AX CX DX BX BP SI DI R8 R9 R10 R11 R12 R13 R15
+				{0, 4295032831}, // AX CX DX BX SP BP SI DI R8 R9 R10 R11 R12 R13 g R15 SB
+			},
+		},
+	},
+	{
+		name:           "LoweredSimdSaturatingSub8x16",
+		argLen:         4,
+		faultOnNilArg0: true,
+		hasSideEffects: true,
+		reg: regInfo{
+			inputs: []inputInfo{
+				{1, 49135},      // AX CX DX BX BP SI DI R8 R9 R10 R11 R12 R13 R15
+				{2, 49135},      // AX CX DX BX BP SI DI R8 R9 R10 R11 R12 R13 R15
+				{0, 4295032831}, // AX CX DX BX SP BP SI DI R8 R9 R10 R11 R12 R13 g R15 SB
+			},
+		},
+	},
 	{
 		name:   "FlagEQ",
 		argLen: 0,
diff --git a/src/cmd/compile/internal/ssa/rewriteAMD64.go b/src/cmd/compile/internal/ssa/rewriteAMD64.go
index ba71189703..3d9dc8ffd8 100644
--- a/src/cmd/compile/internal/ssa/rewriteAMD64.go
+++ b/src/cmd/compile/internal/ssa/rewriteAMD64.go
@@ -1092,6 +1092,30 @@ func rewriteValueAMD64(v *Value) bool {
 	case OpSignExt8to64:
 		v.Op = OpAMD64MOVBQSX
 		return true
+	case OpSimdAdd8x16:
+		v.Op = OpAMD64LoweredSimdAdd8x16
+		return true
+	case OpSimdAddU8x16:
+		v.Op = OpAMD64LoweredSimdAdd8x16
+		return true
+	case OpSimdSaturatingAdd8x16:
+		v.Op = OpAMD64LoweredSimdSaturatingAdd8x16
+		return true
+	case OpSimdSaturatingAddU8x16:
+		v.Op = OpAMD64LoweredSimdSaturatingAddU8x16
+		return true
+	case OpSimdSaturatingSub8x16:
+		v.Op = OpAMD64LoweredSimdSaturatingSub8x16
+		return true
+	case OpSimdSaturatingSubU8x16:
+		v.Op = OpAMD64LoweredSimdSaturatingSubU8x16
+		return true
+	case OpSimdSub8x16:
+		v.Op = OpAMD64LoweredSimdSub8x16
+		return true
+	case OpSimdSubU8x16:
+		v.Op = OpAMD64LoweredSimdSub8x16
+		return true
 	case OpSlicemask:
 		return rewriteValueAMD64_OpSlicemask(v)
 	case OpSpectreIndex:
diff --git a/src/cmd/compile/internal/ssagen/ssa.go b/src/cmd/compile/internal/ssagen/ssa.go
index 760a7f8d06..899b5e890d 100644
--- a/src/cmd/compile/internal/ssagen/ssa.go
+++ b/src/cmd/compile/internal/ssagen/ssa.go
@@ -4293,37 +4293,37 @@ func InitTables() {
 			s.vars[memVar] = s.newValue4(ssa.OpSimdAdd8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
 			return nil
 		},
-		sys.ARM64)
+		sys.AMD64, sys.ARM64)
 	addF("runtime/internal/simd", "SaturatingAdd8x16",
 		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
 			s.vars[memVar] = s.newValue4(ssa.OpSimdSaturatingAdd8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
 			return nil
 		},
-		sys.ARM64)
+		sys.AMD64, sys.ARM64)
 	addF("runtime/internal/simd", "SaturatingAddU8x16",
 		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
 			s.vars[memVar] = s.newValue4(ssa.OpSimdSaturatingAddU8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
 			return nil
 		},
-		sys.ARM64)
+		sys.AMD64, sys.ARM64)
 	addF("runtime/internal/simd", "Sub8x16",
 		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
 			s.vars[memVar] = s.newValue4(ssa.OpSimdSub8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
 			return nil
 		},
-		sys.ARM64)
+		sys.AMD64, sys.ARM64)
 	addF("runtime/internal/simd", "SaturatingSub8x16",
 		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
 			s.vars[memVar] = s.newValue4(ssa.OpSimdSaturatingSub8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
 			return nil
 		},
-		sys.ARM64)
+		sys.AMD64, sys.ARM64)
 	addF("runtime/internal/simd", "SaturatingSubU8x16",
 		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
 			s.vars[memVar] = s.newValue4(ssa.OpSimdSaturatingSubU8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
 			return nil
 		},
-		sys.ARM64)
+		sys.AMD64, sys.ARM64)
 	addF("runtime/internal/simd", "And8x16",
 		func(s *state, n *ir.CallExpr, args []*ssa.Value) *ssa.Value {
 			s.vars[memVar] = s.newValue4(ssa.OpSimdAnd8x16, types.TypeMem, args[0], args[1], args[2], s.mem())
@@ -5123,14 +5123,14 @@ func InitTables() {
 	alias("runtime/internal/sys", "OnesCount64", "math/bits", "OnesCount64", all...)
 
 	/******** simd ********/
-	alias("simd", "addU8x16", "runtime/internal/simd", "Add8x16", sys.ArchARM64)
-	alias("simd", "add8x16", "runtime/internal/simd", "Add8x16", sys.ArchARM64)
-	alias("simd", "saturatingAddU8x16", "runtime/internal/simd", "SaturatingAddU8x16", sys.ArchARM64)
-	alias("simd", "saturatingAdd8x16", "runtime/internal/simd", "SaturatingAdd8x16", sys.ArchARM64)
-	alias("simd", "subU8x16", "runtime/internal/simd", "Sub8x16", sys.ArchARM64)
-	alias("simd", "sub8x16", "runtime/internal/simd", "Sub8x16", sys.ArchARM64)
-	alias("simd", "saturatingSubU8x16", "runtime/internal/simd", "SaturatingSubU8x16", sys.ArchARM64)
-	alias("simd", "saturatingSub8x16", "runtime/internal/simd", "SaturatingSub8x16", sys.ArchARM64)
+	alias("simd", "addU8x16", "runtime/internal/simd", "Add8x16", sys.ArchARM64, sys.ArchAMD64)
+	alias("simd", "add8x16", "runtime/internal/simd", "Add8x16", sys.ArchARM64, sys.ArchAMD64)
+	alias("simd", "saturatingAddU8x16", "runtime/internal/simd", "SaturatingAddU8x16", sys.ArchARM64, sys.ArchAMD64)
+	alias("simd", "saturatingAdd8x16", "runtime/internal/simd", "SaturatingAdd8x16", sys.ArchARM64, sys.ArchAMD64)
+	alias("simd", "subU8x16", "runtime/internal/simd", "Sub8x16", sys.ArchARM64, sys.ArchAMD64)
+	alias("simd", "sub8x16", "runtime/internal/simd", "Sub8x16", sys.ArchARM64, sys.ArchAMD64)
+	alias("simd", "saturatingSubU8x16", "runtime/internal/simd", "SaturatingSubU8x16", sys.ArchARM64, sys.ArchAMD64)
+	alias("simd", "saturatingSub8x16", "runtime/internal/simd", "SaturatingSub8x16", sys.ArchARM64, sys.ArchAMD64)
 	alias("simd", "andU8x16", "runtime/internal/simd", "And8x16", sys.ArchARM64)
 	alias("simd", "and8x16", "runtime/internal/simd", "And8x16", sys.ArchARM64)
 	alias("simd", "orU8x16", "runtime/internal/simd", "Or8x16", sys.ArchARM64)
